{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# there are lots of deliverable projects -- \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n# load with the several data flow algorithms --> with all the loadable values |-->  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nimport requests\nimport json\nimport datetime\nfrom datetime import datetime\nfrom tensorflow import keras # some  required vision for the running the algorithms -- \n# linux time stamp is applied in this  time series \n# json and requests libraries needs to be imported before hand\n# cryptocompare credentials for  this part of the functionis very optional to run -- so needs to load all the rest of the cases .. it \n\n#url = \"https://min-api.cryptocompare.com/data/v2/histominute?fsym=BTC&tsym=USD&limit=10\"\n#apikey = \"424e0c65bd9be940591fe30fae4a23fdfce9e192eaa09e991f65aad43285277b\"\n# run all the rest of the methods .. \n#params = {\"key\":apikey}\n#response = requests.get(url, params)\n# run all of those values that run in the sins\n# form of the potential energy based on the fragility and dependency on the electronic systems .. \n# two different scaler making sure to get loaded on the data frame --. \ndef get_historical_hourly():\n    \"\"\"\n    goal is that one side stationary to predict the price values for the following  time stamp with not high precision\n    \n    \"\"\"\n    \n    print(\"Which coins table data do you want to have?\")\n    #fsym = input()\n    fsym = \"MATIC\"\n    print(\"How many minutes of data in from the past do you want to extract?\")\n    #limit = input() # limit is a number of minutes that program will extract data  -->\n    limit = \"2000\"\n    url = \"https://min-api.cryptocompare.com/data/v2/histohour?fsym=\"+fsym+\"&tsym=USDT&limit=\"+limit\n    \n    apikey = \"424e0c65bd9be940591fe30fae4a23fdfce9e192eaa09e991f65aad43285277b\"\n    params = {\"key\":apikey}\n    response = requests.get(url, params)\n    temp = response.text\n    values = json.loads(temp)\n    return values[\"Data\"][\"Data\"]\ndef get_historical_daily():\n    # it is interesting to download by hourly or daily that can be newer version will be applied that can't be changed -->\n    # access whatever happens to the world that is mattering --> \n    \"\"\"\n    goal is that one side stationary to predict the price values for the following  time stamp with not high precision\n    \n    \"\"\"\n    \n    print(\"Which coins table data do you want to have?\")\n    #fsym = input()\n    fsym = \"MATIC\"\n    print(\"How many minutes of data in from the past do you want to extract?\")\n    #limit = input() # limit is a number of minutes that program will extract data  -->\n    limit = \"2000\"\n    url = \"https://min-api.cryptocompare.com/data/v2/histominute?fsym=\"+fsym+\"&tsym=USDT&limit=\"+limit\n    \n    apikey = \"424e0c65bd9be940591fe30fae4a23fdfce9e192eaa09e991f65aad43285277b\"\n    params = {\"key\":apikey}\n    response = requests.get(url, params)\n    temp = response.text\n    values = json.loads(temp)\n    return values[\"Data\"][\"Data\"]\n\ndef tabledatabuilder(data):\n    \"\"\"\n    this method will get the collecion of the dictionary  and return the tabular data on that -->\n\n    \"\"\"\n    columns = [\"time\", \"high\", \"low\", \"volumefrom\", \"volumeto\", \"close\"]\n    price_minutes = pd.DataFrame(columns = [\"time\", \"high\", \"low\", \"volumefrom\", \"volumeto\", \"close\"])\n    time =  []\n    high_prices = []\n    low_prices =  []\n    volumefrom = []\n    volumeto = []\n    close_prices = []\n    standard_time =  []\n\n    for eachminutedata in data:\n        time.append(eachminutedata[\"time\"])\n        high_prices.append(eachminutedata[\"high\"])\n        low_prices.append(eachminutedata[\"low\"])\n        volumefrom.append(eachminutedata[\"volumefrom\"])\n        volumeto.append(eachminutedata['volumeto'])\n        close_prices.append(eachminutedata['close'])\n        standard_time.append(datetime.fromtimestamp(eachminutedata[\"time\"]))\n    price_minutes['time'] = time\n    price_minutes[\"high\"] = high_prices\n    price_minutes['low'] = low_prices\n    price_minutes[\"volumefrom\"] = volumefrom\n    price_minutes['volumeto'] = volumeto\n    price_minutes[\"close\"] = close_prices\n    price_minutes[\"standardtime\"] = standard_time\n    return price_minutes\n\n\n\n\ndef get_historical():\n    \"\"\"\n    goal is that one side stationary to predict the price values for the following  time stamp with not high precision\n    \n    \"\"\"\n    \n    print(\"Which coins table data do you want to have?\")\n    #fsym = input()\n    fsym = \"MATIC\"\n    print(\"How many minutes of data in from the past do you want to extract?\")\n    #limit = input() # limit is a number of minutes that program will extract data  -->\n    limit = \"2000\"\n    url = \"https://min-api.cryptocompare.com/data/v2/histohour?fsym=\"+fsym+\"&tsym=USDT&limit=\"+limit\n    \n    apikey = \"424e0c65bd9be940591fe30fae4a23fdfce9e192eaa09e991f65aad43285277b\"\n    params = {\"key\":apikey}\n    response = requests.get(url, params)\n    temp = response.text\n    values = json.loads(temp)\n    return values[\"Data\"][\"Data\"]\n\ndata = get_historical()\nprint(data[-1]['time'])\ndata = tabledatabuilder(data)\nsorteddata = data.sort_values(\"time\", ascending = False)\n# can make the data that is called by  the time and day of the year --> \ndata.to_csv(\"data_matic_hourly.csv\", index = False, encoding = \"utf-8\")\nprint(data.shape)\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_member(np_series, value):\n    \"\"\"\n    np_series is the 1 element -->\n    adds a values from the last element - for the np.series -->  that is possible to \n    all index is shifting by 1 to the the end --> \n    \"\"\"\n    np_series = np_series[1:]\n    np_series = list(np_series)\n    np_series.append(value)\n    np_series = np.array(np_series)\n    np_series = np_series.reshape(1, np_series.shape[0], 1)\n    return np_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# where can we see the values -- that are prevalent to the  training for the learning data_sets\n# how is it possible to laod --> rest or all the elements in the datasets.\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\n# load the data - \nscaler = MinMaxScaler(feature_range = (0, 1))\ndata = pd.read_csv(\"../working/data_matic_hourly.csv\")\nprint(data.shape)\ndata[\"difference\"] = data[\"high\"] - data[\"low\"]\ndata_high = data[[\"high\"]].values\ndata_low = data[[\"low\"]].values # high and low will be correlated with each other to run the program --> \n# some normalization for the data --. \nscaled_high = scaler.fit_transform(data_high)\nall_data = []\ntarget = []\nall_data_low = []\ntarget_low = []\n\nfor i in range(168, scaled_high.shape[0]):\n    all_data.append(scaled_high[i-168:i, 0])\n    target.append(scaled_high[i, 0])\n    #all_data_low.append(scale)\nall_data = np.array(all_data)\ntarget = np.array(target)\n\n\ncols = []\n\nfor i in range(all_data.shape[1]):\n    temp =  \"col\"+ str(i)\n    cols.append(temp)\nprint(len(cols))\n#cols\n\n# in order to see the correlation graph --> building the data frame is very critital\ncurrent_df = pd.DataFrame(data = all_data, columns = cols)\n#current_df.corr()# there is significant correlation for the data --> \ncurrent_df.head()\ntarget = np.array(target.reshape(target.shape[0], 1))\nall_data = all_data.reshape(all_data.shape[0], all_data.shape[1], 1)\nall_data_train = all_data[:1500]\nall_data_test = all_data[1500:]\ntrain_target = target[:1500]\ntest_target = target[1500:]\n\n\nprint(all_data_train.shape, train_target.shape, all_data_test.shape, test_target.shape) # this is \n\n# values should be called with those --. \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what kind of engineering works are great fit for loading the datasets -- all around here to maintain higher performance -- metrics --> \n# model creation for this is --> simple but --> prevalent to build it .\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.LSTM(units = 100, return_sequences = True, input_shape = (all_data.shape[1], 1)))\nmodel.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel.add(keras.layers.LSTM(units = 100))\nmodel.add(keras.layers.Dense(units = 1))\nmodel.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 40 # this is optimized hyper parameter\n# for the test just train within in a single epochs --> \n# 7 days --> correlations -- trigger  the generality \n# batch_size -- smaller is  slower training through the details\nhistory  = model.fit(all_data_train, train_target, epochs= epochs, batch_size=32, validation_data= (all_data_test, test_target)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"history  = model.fit(all_data_train, train_target, epochs= 100, batch_size=100, validation_data= (all_data_test, test_target)) "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# there should be some loads for the datasets -->  that is running on the current service --> \n# all the correlation -- within the data --> \n# can reset the model to run --> build on it. cdc -- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Loss function representation \") # this is the scaled loss functions -- that is running -- \nimport matplotlib.pyplot as plt\nplt.plot(history.history[\"loss\"])# can give the color coding here to build \nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"training_loss\", \"validation_loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntest and training data_split approximate in size  -- similar loss values for good model.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# not required to plot with this->\nplt.plot(history.history[\"val_loss\"]) #the validation loss -- will be updated when values  will be prevalent.\n# there is signicant drop in the performance of the model --. when it runs on the server\n# by showing the values the proper epochs would be 40\nplt.show()\n# by particular point -- required to have -- the scatter values --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.save(\"model_high_with_30_epochs.pb\")\n# there is some tendency for the graph that is missing very high defferentiation --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# when the program runs -- i will get the highest -- points --> \n# some components of the program will be suffled to run -","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\nvalue = model.predict(current)\npredicted = scaler.inverse_transform(value)\n\n\nts = list(current.reshape(current.shape[1]))\n\nts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \nts = np.array(ts)\nts = ts.reshape(ts.shape[0], 1)\nts = scaler.inverse_transform(ts)\nts_high = ts.reshape(ts.shape[0])\nprint(ts_high.shape)\nplt.plot(ts_high[-20:]) # some outliers -->  if it represents the \nplt.show()\npredicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\ndef buildgraph(current, model):\n    \"\"\"\n    This will get the particular time series then returns - the values -- useful for the next prediciton.\n    current is test set to get the one stop data precition value -- np.array 1d\n    \n    \"\"\"\n    value = model.predict(current)\n    predicted = scaler.inverse_transform(value)\n\n    ts = list(current.reshape(current.shape[1]))\n\n    ts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \n    ts = np.array(ts)\n    #ts = ts.reshape(ts.shape[0], 1)\n    #ts = scaler.inverse_transform(ts)\n    ts_high = ts\n    ts_high = ts_high[1:]# comes into the the  same length of row features\n    \n    print(ts_high.shape)\n    \n    \n    plt.plot(ts_high[-20:]) # some outliers -->  if it represents the \n    plt.legend([\"prices_scaled\"])\n    plt.show()\n    ts_high  = ts_high.reshape(1, ts_high.shape[0], 1)\n    \n    \n    return ts_high, predicted\n\n\n    \n    \n    \n    #ts = ts[1:]\n    #ts = list(ts)\n    #ts.append(value)\n    #ts = np.array(ts)\n    #ts = ts.reshape(1, ts.shape[0], 1)\n    #return ts\npredicted_steps = []\nfor i in range(10): # get 10 steps\n    current, predicted_s = buildgraph(current, model)\n    print(predicted_s)\n    predicted_steps.append(predicted_s)\n    # running for forecast of th 3 stage predictions --> \n    # can use the last real values.\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_steps # exposing --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# where can we see the values -- that are prevalent to the  training for the learning data_sets\n# how is it possible to laod --> rest or all the elements in the datasets.\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\n# load the data - \nscaler = MinMaxScaler(feature_range = (0, 1))\ndata = pd.read_csv(\"../working/data_matic_hourly.csv\")\nprint(data.shape)\ndata[\"difference\"] = data[\"high\"] - data[\"low\"]\ndata_high = data[[\"low\"]].values\ndata_low = data[[\"low\"]].values # high and low will be correlated with each other to run the program --> \n# some normalization for the data --. \nscaled_high = scaler.fit_transform(data_high)\nall_data = []\ntarget = []\nall_data_low = []\ntarget_low = []\n\nfor i in range(168, scaled_high.shape[0]):\n    all_data.append(scaled_high[i-168:i, 0])\n    target.append(scaled_high[i, 0])\n    #all_data_low.append(scale)\nall_data = np.array(all_data)\ntarget = np.array(target)\n\n\n\ncols = []\n\n#temporary data load and validation that can clearly be the higher performance loads -- withall the rest of the variables.\n\n\nfor i in range(all_data.shape[1]):\n    temp =  \"col\"+ str(i)\n    cols.append(temp)\nprint(len(cols))\n#cols\n\n# in order to see the correlation graph --> building the data frame is very critital\ncurrent_df = pd.DataFrame(data = all_data, columns = cols)\n#current_df.corr()# there is significant correlation for the data --> \ncurrent_df.head()\ntarget = np.array(target.reshape(target.shape[0], 1))\nall_data = all_data.reshape(all_data.shape[0], all_data.shape[1], 1)\nall_data_train = all_data[:1500]\nall_data_test = all_data[1500:]\ntrain_target = target[:1500]\ntest_target = target[1500:]\n\n\nprint(all_data_train.shape, train_target.shape, all_data_test.shape, test_target.shape) # this is ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# model creation for this is --> simple but --> prevalent to build it .\nmodel1 = keras.models.Sequential()\nmodel1.add(keras.layers.LSTM(units = 100, return_sequences = True, input_shape = (all_data.shape[1], 1)))\nmodel1.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel1.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel1.add(keras.layers.LSTM(units = 100))\nmodel1.add(keras.layers.Dense(units = 1))\nmodel1.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\nmodel1.summary() # there is the summary of the model that can be loaded into a several values.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it is really important to make the current graph to the default --  \n# significance correlation graph \nepochs = 50 # setting this epochs into the  50 then -- loading  with the lower batch_size as with hyper parameter  optimization --> \nhistory = model1.fit(all_data_train, train_target, epochs= epochs, batch_size=100, validation_data= (all_data_test, test_target)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Loss function representation \") # this is the scaled loss functions -- that is running -- \nimport matplotlib.pyplot as plt\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"training_loss\", \"validation_loss\"])\n\nplt.show()\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"validation_loss\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\ndef buildgraph(current, model):\n    \"\"\"\n    This will get the particular time series then returns - the values -- useful for the next prediciton.\n    current is test set to get the one stop data precition value -- np.array 1d\n    \n    \"\"\"\n    value = model.predict(current)\n    predicted = scaler.inverse_transform(value)# predicted is already scaled\n\n    ts = list(current.reshape(current.shape[1]))\n\n    ts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \n    ts = np.array(ts)\n    #ts = ts.reshape(ts.shape[0], 1)\n    #ts = scaler.inverse_transform(ts)\n    ts_high = ts\n    ts_high = ts_high[1:]# comes into the the  same length of row features\n    \n    print(ts_high.shape)\n    \n    \n    plt.plot(ts_high[-20:]) # some outliers -->  if it represents the \n    plt.legend([\"prices_scaled\"])\n    plt.show()\n    ts_high  = ts_high.reshape(1, ts_high.shape[0], 1)\n    \n    \n    return ts_high, predicted \n    \n    #ts = ts[1:]\n    #ts = list(ts)\n    #ts.append(value)\n    #ts = np.array(ts)\n    #ts = ts.reshape(1, ts.shape[0], 1)\n    #return ts\npredicted_steps_low = []\nfor i in range(10):\n    current, predicted_s = buildgraph(current, model1)\n    print(predicted_s)\n    predicted_steps_low.append(predicted_s)\n    # running for forecast of th 3 stage predictions --> \n    # can use the last real values.\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"highest_predicted = []\nfor i in predicted_steps:\n    highest_predicted.append(i[0][0])# there\n\nlowest_predicted = []\nfor i in predicted_steps_low:\n    lowest_predicted.append(i[0][0])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rerscale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ncurrent = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\nvalue = model1.predict(current)\npredicted = scaler.inverse_transform(value)\n\n\nts = list(current.reshape(current.shape[1]))\n\nts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \nts = np.array(ts)\nts = ts[1:]\nts = ts.reshape(1, ts.shape[0], 1)\nvalue = model1.predict(ts)\npredicted1 = scaler.inverse_transform(value)\n#ts = scaler.inverse_transform(value)\nts_high = ts.reshape(ts.shape[0])\n# there are several functions that can run with higher performance metrics -->  \nprint(predicted, predicted1)\nts = list(current.reshape(current.shape[1]))\nplt.plot(ts_high[-20:]) # some outliers -->  if it represents the \nplt.show()\n\n \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(highest_predicted)\nplt.plot(lowest_predicted)\nplt.legend([\"highest\", \"lowest\"])\nplt.title(\"hourly prices for next 10 hrs\")\nplt.xlabel(\"time points\")\nplt.ylabel(\"prices\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[-20:][[\"high\", \"low\"]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ntime.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}